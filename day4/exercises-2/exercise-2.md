# Exercise preprocessing and vectorization of text.


Try to take some of the data from the exercise of this morning.

- preprocess them (in different ways)

#- give a (tabular and/or graphical) overview of tokens (unigrams, bigrams, collocations)


Then,

- vectorize them

- try out a simple supervised model: Can you predict source (i.e., outlet of the news articles) using textual features? Define different vectorizers for this task (`count` vs. `tfidf`)


- compare that bottom-up approach with a top-down (keyword or regular-expression based) approach
