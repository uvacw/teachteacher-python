{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b204acf-e794-46d3-99c0-390a4ebb101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exercise 2\n",
    "# Getting more hands-on experience with supervised machine learning\n",
    "\n",
    "## Question 1. \n",
    "In exercise 1, you set up a classifier using the hatespeech dataset (retrieved from: https://www.dropbox.com/sh/4mapojr85a6sc76/AABYMkjLVG-HhueAgd0qM9kwa?dl=0![image-2.png](attachment:image-2.png))\n",
    "The classifier was based on a count vectorizer using Naïve Bayes. For this exercise, you created the following code:\n",
    "\n",
    "```python\n",
    "import csv\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "file = \"hatespeech_text_label_vote_RESTRICTED_100K.csv\"\n",
    "tweets = []\n",
    "labels = []\n",
    "\n",
    "with open(file) as fi:\n",
    "    data = csv.reader(fi, delimiter='\\t')\n",
    "    for row in data:\n",
    "        tweets.append(row[0])\n",
    "        labels.append(row[1])\n",
    "\n",
    "print(len(tweets) == len(labels)) # there should be just as many tweets as there are labels\n",
    "\n",
    "Counter(labels)\n",
    "plt.bar(Counter(labels).keys(), Counter(labels).values())\n",
    "\n",
    "\n",
    "# splitting up the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tweets_train, tweets_test, y_train, y_test = train_test_split(tweets, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Classifier with a count vectorizer and Naïve Bayes\n",
    "from sklearn.feature_extraction.text import (CountVectorizer)\n",
    "\n",
    "countvectorizer = CountVectorizer(stop_words=\"english\")\n",
    "X_train = countvectorizer.fit_transform(tweets_train)\n",
    "X_test = countvectorizer.transform(tweets_test)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "\n",
    "# Print a classification report for the classifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Can you use the examples from the slides used in the lecture and train another classifier based on Logistic Regression and a tf-idf vectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac96a66-12b0-4bb2-88ba-032b00a27109",
   "metadata": {},
   "source": [
    "## Question 2.\n",
    "\n",
    "As discussed earlier, we can try different combinations of these models (Naïve Bayes and Logistic Regression) and vectorizers (count and tf-idf). If you want to use Naïve Bayes and Logistic Regression as the models for a classifier, and a count vectorizer and a tf-idf vectorizer, how many classifiers could you then train? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e29260",
   "metadata": {},
   "source": [
    "## Question 3.\n",
    "\n",
    "We could simply copy-paste the code used in the previous questions and adjust it for each of the classifiers. However, a cleaner approach is to write a function in which we define the specifics of each classifier. The code below does that.\n",
    "\n",
    "In this code, we create a loop that trains each classifier by calling the function that is build in the first part of the code. \n",
    "\n",
    "Run the code below and compare it to the code that you wrote to train one classifier: Do you understand what is happening there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09348f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB-count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.80      0.88      0.84      5369\n",
      "     hateful       0.41      0.28      0.33       966\n",
      "      normal       0.85      0.79      0.82     10848\n",
      "        spam       0.53      0.63      0.57      2817\n",
      "\n",
      "    accuracy                           0.77     20000\n",
      "   macro avg       0.65      0.64      0.64     20000\n",
      "weighted avg       0.77      0.77      0.77     20000\n",
      "\n",
      "\n",
      "\n",
      "NB-TfIdf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.81      0.81      0.81      5369\n",
      "     hateful       0.87      0.05      0.09       966\n",
      "      normal       0.76      0.92      0.83     10848\n",
      "        spam       0.65      0.32      0.43      2817\n",
      "\n",
      "    accuracy                           0.77     20000\n",
      "   macro avg       0.77      0.53      0.54     20000\n",
      "weighted avg       0.76      0.77      0.73     20000\n",
      "\n",
      "\n",
      "\n",
      "LR-Count\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.87      0.91      0.89      5369\n",
      "     hateful       0.63      0.28      0.38       966\n",
      "      normal       0.81      0.91      0.86     10848\n",
      "        spam       0.58      0.39      0.47      2817\n",
      "\n",
      "    accuracy                           0.80     20000\n",
      "   macro avg       0.72      0.62      0.65     20000\n",
      "weighted avg       0.79      0.80      0.79     20000\n",
      "\n",
      "\n",
      "\n",
      "LR-TfIdf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     abusive       0.88      0.89      0.89      5369\n",
      "     hateful       0.73      0.20      0.32       966\n",
      "      normal       0.80      0.94      0.86     10848\n",
      "        spam       0.66      0.35      0.46      2817\n",
      "\n",
      "    accuracy                           0.81     20000\n",
      "   macro avg       0.77      0.60      0.63     20000\n",
      "weighted avg       0.80      0.81      0.78     20000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import (CountVectorizer)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import (TfidfVectorizer)\n",
    "from sklearn.linear_model import (LogisticRegression)\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "configs = [\n",
    "  (\"NB-count\",CountVectorizer(min_df=5,max_df=.5),\n",
    "   MultinomialNB()),\n",
    "  (\"NB-TfIdf\",TfidfVectorizer(min_df=5,max_df=.5),\n",
    "   MultinomialNB()),\n",
    "  (\"LR-Count\",CountVectorizer(min_df=5,max_df=.5),\n",
    "   LogisticRegression(solver=\"liblinear\")),\n",
    "  (\"LR-TfIdf\",TfidfVectorizer(min_df=5,max_df=.5),\n",
    "   LogisticRegression(solver=\"liblinear\"))]\n",
    "\n",
    "\n",
    "for name, vectorizer, classifier in configs:\n",
    "    print(name)\n",
    "    X_train = vectorizer.fit_transform(tweets_train)\n",
    "    X_test = vectorizer.transform(tweets_test)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"\\n\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c8380",
   "metadata": {},
   "source": [
    "## Question 4.\n",
    "\n",
    "Check out the documentation of scikit learn (https://scikit-learn.org/stable/supervised_learning.html). Can you try to use other models and train a classifier with them? Can you merge this code into the code used in the previous question?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf209f7e",
   "metadata": {},
   "source": [
    "## Question 5.\n",
    "\n",
    "Based on the output that the classifier prints, what classifier performs the best? In your answer, consider:\n",
    "* What information you need to identify the best classifier\n",
    "* What metric you base your conclusion (i.e., precision, recall, accurcay, or F1-score) on and why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977fce73",
   "metadata": {},
   "source": [
    "## Question 6.\n",
    "\n",
    "Let's say that you base your evaluation on the F1-score of the classifier. You can choose between the macro average and the weighted average of the F1-score. Check out the scikit learn documentation (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support). What F1-value (macro average or weighted average) would you select?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92830270",
   "metadata": {},
   "source": [
    "## Question 7.\n",
    "\n",
    "When looking at the classification report, you will see another column indicating values for something labelled 'support'.\n",
    "Can you do some searching online and find out what 'support' is?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afe8fa3",
   "metadata": {},
   "source": [
    "## Question 8.\n",
    "\n",
    "Two researchers want to use the classifier to distinguish between tweets that are spam or hateful and tweets that are not (either because they are normal or abusive). \n",
    "They are, however, not happy with the performance of the classifier when looking at the accuracy, precision, and recall for the spam category or the hateful category.\n",
    "One of the researchers suggests to first recode the labels, so that all tweets that were annotated as spam receive a label 'spam' or 'hateful' are grouped together and all other tweets are grouped together as well.\n",
    "\n",
    "What would the consequences be of doing so?\n",
    "What can you do to check your own answer? Try to recode the labels and see what happens!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80781bc5",
   "metadata": {},
   "source": [
    "## Question 9.\n",
    "For now, let's say that the classifier based on a count vectorizer and Logistic Regression is the one we prefer. We now want to use this model to predict the label for new data that we have not annotated (remember, this was the whole goal of SML)!\n",
    "\n",
    "To do this, let’s save our classifier and our vectorizer to a file. If we don’t do this, we would need to re-train our model every time we want to use it. This is not so convenient, for example, we would always need to have our training data at hand. The code below shows you how to make a vectorizer and train a classifier (a repetition of what we did before to show you the whole process) and store them into a file.\n",
    "\n",
    "In the code, you will see that both the classifier and the vectorizer are stored into a file. Why do you need to store both (why not just store the classifier only)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "858d4a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This Tweet is very shitty nasty mean and hateful' is probably 'abusive'.\n",
      "'This is a very normal normal tweet.' is probably 'normal'.\n",
      "'2%^&GHJ &(&hrqjf3 click this link' is probably 'spam'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Make a vectorizer and train a classifier\n",
    "vectorizer=CountVectorizer(min_df=5, max_df=.5)\n",
    "classifier=LogisticRegression(solver=\"liblinear\")\n",
    "X_train=vectorizer.fit_transform(tweets_train)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Save them to disk\n",
    "with open(\"myvectorizer.pkl\",mode=\"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "with open(\"myclassifier.pkl\",mode=\"wb\") as f:\n",
    "    joblib.dump(classifier, f)\n",
    "\n",
    "# Later on, re-load this classifier and apply:\n",
    "new_tweets = [\"This Tweet is very shitty nasty mean and hateful\", \n",
    "            \"This is a very normal normal tweet.\", \n",
    "            \"2%^&GHJ &(&hrqjf3 click this link\"]\n",
    "\n",
    "with open(\"myvectorizer.pkl\",mode=\"rb\") as f:\n",
    "    myvectorizer = pickle.load(f)\n",
    "with open(\"myclassifier.pkl\",mode=\"rb\") as f:\n",
    "    myclassifier = joblib.load(f)\n",
    "    \n",
    "new_features = myvectorizer.transform(new_tweets)\n",
    "pred = myclassifier.predict(new_features)\n",
    "\n",
    "for tweet, label in zip(new_tweets, pred):\n",
    "    print(f\"'{tweet}' is probably '{label}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930d01dc",
   "metadata": {},
   "source": [
    "### About this exercise:\n",
    "\n",
    "This exercise is based on the materials developed and the texts written by Wouter van Atteveldt, Damian Trilling and Carlos Arcila Calderon as reported in their book 'Computational Analysis of Communication' (2022, Wiley-Blackwell)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
